---
title: "Boosted Decision Trees: Learn from Mistakes and Predict with Power"
excerpt: "Boosted Decision Tree Algorithm"
# permalink: /courses/ensemble/boosteddt/
last_modified_at: 2022-01-22T23:45:00-00:00
classes: narrow
hidden: false
strip_title: true
categories: 
  - Tree Model
  - Ensemble Model
  - Boosting
  - Supervised Learning
  - Classification Algorithm
  - Regression Algorithm
tags: 
  - Ensemble methods
  - Boosting
  - Tree Models
  - Classification algorithm
  - Regression algorithm
---

{% include download file="boosted_dt_example.py" alt="Download Boosted Decision Tree Code" text="Download Code" %}

## 1. Introduction to Boosted Decision Trees

Imagine you're trying to teach someone to identify different types of birds. You start by showing them one characteristic, like size: "Big birds are eagles." They might get some right, but will definitely make mistakes (like confusing hawks with eagles). Then, you correct them and give them another clue: "Eagles have broad wings, unlike hawks."  With each new clue and correction, they get better and better at distinguishing birds.

**Boosted Decision Trees** work in a similar way! They are a type of **ensemble learning** algorithm, meaning they combine multiple simpler models to make more accurate predictions. In this case, the simpler models are **decision trees**, and the 'boosting' part means they learn step-by-step, focusing on correcting the mistakes made by previous steps.

Think of it like building a prediction model in rounds. In each round, a new decision tree is created that tries to fix the errors of the combined model from all previous rounds. It's like having a team of decision trees, where each new tree is a specialist focusing on where the previous trees went wrong.

**Why "Boosted Decision Trees"?**

*   **Boosted:**  Refers to the boosting technique, where models are built sequentially, with each model trying to improve upon the weaknesses of the ensemble built so far. Itâ€™s like boosting the performance of the overall model iteratively.
*   **Decision Trees:**  The basic building blocks are decision trees, which are simple models that make decisions based on a series of questions.

**Real-world examples where Boosted Decision Trees are used:**

*   **Spam Email Detection:** Imagine email providers using boosted decision trees to filter spam. The algorithm learns patterns from email content and sender information to identify spam emails, constantly improving its accuracy as it learns from new emails and user feedback.
*   **Medical Diagnosis:**  Boosted decision trees can assist doctors in diagnosing diseases. By analyzing patient symptoms, medical history, and test results, these models can predict the likelihood of a specific condition, aiding in earlier and more accurate diagnoses.
*   **Recommendation Systems:**  When you get product recommendations online, boosted decision trees might be at play. These models can predict what you might like based on your past behavior, preferences, and the behavior of similar users, refining their suggestions over time.
*   **Fraud Detection in Finance:** Banks and financial institutions use boosted decision trees to detect fraudulent transactions. The models learn patterns from transaction data to identify suspicious activities, becoming more effective at spotting fraud as they are trained on more transaction history.

Boosted Decision Trees are popular because they often provide high accuracy, can handle different types of data, and are relatively robust. They are a go-to algorithm for many machine learning tasks, especially when dealing with structured data.

## 2. Mathematics Behind Boosted Decision Trees

Let's break down the mathematics behind Boosted Decision Trees in a way that's easy to grasp. The core idea is iterative error correction using decision trees.

**Key Concepts:**

1.  **Weak Learners (Decision Trees):** Boosted Decision Trees use many simple decision trees, often called "weak learners" or "base learners." These trees are usually shallow (meaning they don't have many layers of decisions), making them individually not very powerful, but computationally efficient.

2.  **Sequential Building:** Unlike algorithms like Random Forests that build trees independently and in parallel, Boosted Decision Trees build trees sequentially. Each new tree is grown based on the performance of the trees built so far.

3.  **Weighted Data Points:** In Boosting, each data point is assigned a weight. Initially, all data points have equal weights. As the algorithm progresses, data points that are incorrectly predicted by the current set of trees get their weights increased, while correctly predicted points get their weights decreased. This way, subsequent trees focus more on the "harder" examples that the model has struggled with.

4.  **Additive Model:** The final prediction is made by summing up the predictions of all the decision trees in the ensemble, with each tree's prediction potentially weighted.

**Algorithm Steps (Simplified):**

Let's say we are trying to predict a target variable $y$ based on features $X$.

1.  **Initialize Weights:** Start by assigning equal weights to all training data points. If we have $N$ data points, the initial weight for each data point $i$ might be $w_i = \frac{1}{N}$.

2.  **Build a Weak Learner (Decision Tree):** Train a decision tree, $h_1(x)$, on the training data, considering the current weights of the data points. The tree is built to best predict the target variable $y$, but with emphasis on the data points that have higher weights (those that were previously misclassified or poorly predicted).

3.  **Calculate Error:** Evaluate the performance of $h_1(x)$ on the training data.  Calculate the error or residuals. For example, in regression, this could be the difference between the actual value and the predicted value. In classification, it could be whether a point was correctly or incorrectly classified.

4.  **Update Data Point Weights:** Increase the weights of the data points that were poorly predicted by $h_1(x)$. Decrease the weights of the data points that were predicted well. This makes the algorithm focus more on the difficult cases in the next iteration.

5.  **Build the Next Weak Learner:** Train another decision tree, $h_2(x)$, on the *same* training data, but now using the *updated weights*.  $h_2(x)$ will be encouraged to focus on the data points that $h_1(x)$ struggled with.

6.  **Combine Predictions:** Add the prediction of $h_2(x)$ to the prediction of $h_1(x)$, potentially with a weighting factor (like a learning rate $\alpha$) to control the contribution of each tree.  So, the combined prediction so far is $F_2(x) = F_1(x) + \alpha \cdot h_2(x)$, where $F_1(x)$ was just the prediction from $h_1(x)$ (or an initial guess in the very first step).

7.  **Repeat Steps 3-6:** Repeat steps 3 through 6 for a predefined number of iterations or until performance improvement plateaus. In each iteration $m$, a new decision tree $h_m(x)$ is trained, weights are updated, and the prediction is updated:

    $F_m(x) = F_{m-1}(x) + \alpha \cdot h_m(x)$

    where $F_{m-1}(x)$ is the combined prediction from the previous $m-1$ trees, and $\alpha$ is the learning rate (a small positive value, like 0.1, that scales down the contribution of each new tree). The learning rate is important to prevent overfitting and to allow the model to learn gradually.

8.  **Final Prediction:** After a certain number of iterations (say, $M$), the final prediction of the Boosted Decision Tree model is the sum of the predictions of all $M$ trees (each scaled by the learning rate, except for the initial prediction if there was one):

    $F_{final}(x) = \sum_{m=1}^{M} \alpha \cdot h_m(x)$  (If starting from a zero initial prediction)

**Mathematical Example (Simplified for Regression):**

Let's say we want to predict house prices (in thousands of dollars) based on size (in sq ft).  Let's take a very simplified scenario with just two houses:

| House | Size (sq ft) ($x$) | Actual Price ($y$) |
|-------|--------------------|----------------------|
| 1     | 1000               | 150                  |
| 2     | 2000               | 250                  |

**Iteration 1:**

*   Initial weights: $w_1 = 0.5, w_2 = 0.5$
*   Train a simple decision tree $h_1(x)$ (e.g., maybe it just predicts the average price, say 200 for both, based on initial weights). Let's say $h_1(x) = 200$ for all $x$.
*   Predictions: $\hat{y}_{1,1} = 200, \hat{y}_{1,2} = 200$
*   Errors (Residuals): $r_{1,1} = 150 - 200 = -50, r_{1,2} = 250 - 200 = 50$
*   Update weights (let's say house 1 was underpredicted, house 2 was overpredicted, so we might increase weight for house 1 and decrease for house 2, in a boosting algorithm usually weights are updated based on error size and direction, using a more formal method than this example shows). Let's just say after weight update: $w_{2,1} = 0.6, w_{2,2} = 0.4$ (weights are re-normalized to sum to 1)

**Iteration 2:**

*   Weights from previous step: $w_{2,1} = 0.6, w_{2,2} = 0.4$
*   Train a new decision tree $h_2(x)$ that now focuses more on house 1 (because it has higher weight). Maybe $h_2(x)$ learns that if size is 1000 sq ft, predict a *correction* of -40, and if size is 2000 sq ft, predict a correction of +30 (again, highly simplified tree for example). So, $h_2(1000) = -40, h_2(2000) = 30$.
*   Predictions from $h_2$: $\hat{y}_{2,1} = -40, \hat{y}_{2,2} = 30$
*   Combined Predictions (with learning rate $\alpha=0.1$, for example):
    $F_2(1000) = F_1(1000) + 0.1 \times h_2(1000) = 200 + 0.1 \times (-40) = 196$
    $F_2(2000) = F_1(2000) + 0.1 \times h_2(2000) = 200 + 0.1 \times (30) = 203$
*   Calculate new residuals, update weights, and continue for more iterations.

With each iteration and tree added, the combined model $F_m(x)$ gets better at predicting the house prices. The "boosting" comes from focusing on the errors and re-weighting data points so subsequent models concentrate on the difficult-to-predict instances.

**Key takeaway:** Boosted Decision Trees build models incrementally by sequentially adding decision trees, where each new tree tries to correct the mistakes of the previous ones, giving more importance to data points that are harder to predict. This iterative refinement and weighted focus on errors is what makes boosting powerful.

## 3. Prerequisites and Preprocessing

Before implementing Boosted Decision Trees, let's look at prerequisites, assumptions, and required tools.

**Prerequisites:**

*   **Understanding of Decision Trees (Basic):** Boosted Decision Trees are built upon decision trees. A foundational understanding of how decision trees work (splitting data based on features, making predictions) is essential.
*   **Basic Machine Learning Concepts:** Familiarity with features, target variable, training data, testing data, and the concept of model training and prediction is needed.
*   **Python Programming:** Code examples will be in Python. Basic Python programming knowledge is required to run and understand the code.

**Assumptions:**

*   **Data in Tabular Format:** Boosted Decision Trees typically work with structured, tabular data (data in rows and columns).
*   **No Strict Statistical Assumptions (Compared to Linear Models):** Unlike linear regression or similar models, Boosted Decision Trees (especially tree-based implementations) are less sensitive to strict assumptions about data distribution, linearity, or feature scaling. They can handle non-linear relationships and complex interactions between features effectively.
*   **Sufficient Data (Generally Beneficial):**  While Boosted Decision Trees can work with smaller datasets, they generally benefit from having a reasonable amount of data to learn complex patterns and avoid overfitting.

**Testing Assumptions (Informally):**

"Testing assumptions" for Boosted Decision Trees is more about checking data suitability rather than formal statistical tests:

*   **Check for Missing Values:** Boosted Decision Trees can handle missing values to varying degrees (depending on the specific implementation and library). Some libraries (like XGBoost, LightGBM, CatBoost) have built-in handling for missing values. However, it's still good practice to understand and possibly address missing data (impute or remove) if it's significant in your dataset.
*   **Examine Data Types:** Ensure your features are of appropriate types (numerical, categorical). Categorical features often need to be encoded into numerical representations for most implementations.
*   **Feature Scaling (Less Critical but Potentially Helpful in some Variations):**  For standard tree-based Boosted Decision Trees, feature scaling (normalization or standardization) is generally **not critically important**. Decision trees make splits based on feature values, and scaling doesn't fundamentally change these splits. However, in some variations of boosting or when using regularization, scaling *might* have a minor impact or improve convergence speed. For most practical purposes, you can often skip feature scaling with Boosted Decision Trees.
*   **Consider Feature Engineering:** Creating new features or transforming existing ones can often have a greater impact on model performance than extensive preprocessing or hyperparameter tuning.

**Python Libraries Required:**

*   **`scikit-learn` (sklearn):**  Fundamental Python library for machine learning. Provides `GradientBoostingClassifier` and `GradientBoostingRegressor` for Boosted Decision Trees. Also includes tools for data splitting, evaluation metrics, and preprocessing.
*   **`numpy`:** For numerical operations, especially with arrays.
*   **`pandas`:** For data manipulation and analysis, particularly working with tabular data in DataFrames.
*   **Specialized Gradient Boosting Libraries (Often for better performance and features):**
    *   **`xgboost` (Extreme Gradient Boosting):** Highly optimized and widely used Gradient Boosting library. `import xgboost as xgb`
    *   **`lightgbm` (Light Gradient Boosting Machine):** Another efficient and fast Gradient Boosting framework. `import lightgbm as lgb`
    *   **`catboost` (Categorical Boosting):**  Designed to handle categorical features effectively and often with less need for preprocessing. `import catboost as cb`

For many practical applications, especially when aiming for top performance, using libraries like `xgboost`, `lightgbm`, or `catboost` is often preferred over `scikit-learn`'s base implementation due to their speed, efficiency, and advanced features.

```python
# Required Python Libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor # scikit-learn implementation
from sklearn.metrics import accuracy_score, mean_squared_error, r2_score
import xgboost as xgb # xgboost library
import lightgbm as lgb # lightgbm library
from catboost import CatBoostClassifier, CatBoostRegressor # catboost library (install separately if needed: pip install catboost)
```

## 4. Data Preprocessing

Data preprocessing is a crucial step for many machine learning algorithms, but for Boosted Decision Trees, the necessity and type of preprocessing depend largely on the specific requirements of the implementation and the characteristics of your data.

**Preprocessing Often Required or Recommended:**

*   **Encoding Categorical Features:** Most standard Boosted Decision Tree implementations work best with numerical input features. If you have categorical features (textual categories, like "color," "city," "product type"), you typically need to convert them into numerical representations. Common methods are:
    *   **One-Hot Encoding:** Creates binary (0/1) columns for each category. For example, a feature "Color" with categories "Red," "Green," "Blue" becomes three binary features: "Is\_Red," "Is\_Green," "Is\_Blue." Suitable for nominal (unordered) categorical features.
    *   **Label Encoding (Ordinal Encoding):** Assigns a unique integer to each category. May be suitable for ordinal categorical features (categories with a meaningful order, like "Low," "Medium," "High"). Be cautious using for nominal features as it implies an order that may not exist.
    *   **Note:** Libraries like CatBoost can handle categorical features directly without explicit encoding, which is a significant advantage.

*   **Handling Missing Values:** While some advanced Boosted Decision Tree libraries (XGBoost, LightGBM, CatBoost) have built-in mechanisms to handle missing values, it's still good practice to understand and address missing data. Strategies include:
    *   **Imputation:** Replacing missing values with estimated values. Common methods are:
        *   **Mean/Median Imputation (Numerical):** Replace missing numerical values with the mean or median of the feature column.
        *   **Mode Imputation (Categorical):** Replace missing categorical values with the most frequent category.
        *   **More sophisticated imputation:** Model-based imputation using KNN or other predictive models.
    *   **Missing Value Indicators:** Create an additional binary feature to indicate whether a value was originally missing. This can help the model learn patterns related to missingness.

**Preprocessing Less Critical or Can Be Ignored in Many Cases:**

*   **Feature Scaling (Normalization/Standardization):** As mentioned before, tree-based models, including Boosted Decision Trees, are generally **not very sensitive to feature scaling**. Decision trees split data based on feature value comparisons, and scaling typically doesn't alter these comparisons.
    *   **Why scaling is often ignored:** Feature scaling primarily helps algorithms that are distance-based or use gradient descent (to speed up convergence or prevent features with larger scales from dominating). Tree-based models are not directly affected by feature scales in the same way.
    *   **When scaling might be considered (less common):** In some hybrid approaches or if you are combining Boosted Decision Trees with other model types in an ensemble where scaling is important for those other models. Or if you are using specific regularization techniques where scaling could have a very minor effect. But for most standard Boosted Decision Tree scenarios, scaling is not a priority.

*   **Outlier Handling:** Boosted Decision Trees, especially tree-based implementations, are reasonably robust to outliers. Decision trees can isolate outliers in specific branches.
    *   **Consider if outliers are problematic:** If outliers are clearly errors or truly unrepresentative, you might consider techniques like outlier removal or capping. If outliers represent genuine extreme values in your data, Boosted Decision Trees can often handle them effectively without explicit outlier preprocessing.

**Examples:**

1.  **Scenario: Predicting customer satisfaction (classification).** Features: "customer age," "number of interactions," "product category (categorical: Electronics, Clothing, Books)," "region (categorical: North, South, East, West)," "average purchase value."

    *   **Preprocessing:**
        *   **Categorical Encoding:** One-hot encode "product category" and "region."
        *   **Missing Values:** Check for missing data in any feature. Decide on imputation strategy if needed (e.g., median imputation for "average purchase value," mode imputation for "region" if it has missing values).
        *   **Feature Scaling:** Usually not necessary for Boosted Decision Trees in this case.

2.  **Scenario: Predicting sales revenue (regression).** Features: "marketing spend," "number of sales reps," "product type (categorical: Type A, Type B, Type C)," "season (categorical: Spring, Summer, Autumn, Winter)," "store location (city names)."

    *   **Preprocessing:**
        *   **Categorical Encoding:** One-hot encode "product type," "season," and "store location." If there are many store locations (high cardinality categorical feature), consider dimensionality reduction techniques or alternative encoding methods if one-hot encoding creates too many features.
        *   **Missing Values:** Handle any missing data in "marketing spend," "number of sales reps," etc. Impute or remove rows/columns depending on the extent and nature of missingness.
        *   **Feature Scaling:** Not typically required for Boosted Decision Trees themselves.

**Decision on Preprocessing:**

*   **Focus on Categorical Encoding and Missing Value Handling:** These are the most important preprocessing steps for Boosted Decision Trees. Ensure categorical features are appropriately encoded, and address missing data in a way that makes sense for your data.
*   **Feature Scaling is Generally Not Necessary:**  You can often skip feature scaling without significant impact on Boosted Decision Tree performance.
*   **Outlier Handling:** Consider if outliers are problematic based on domain knowledge, but Boosted Decision Trees are usually more robust than some other algorithms.
*   **Validate Preprocessing Choices:** Always evaluate model performance (e.g., using cross-validation) with and without different preprocessing steps to determine the optimal approach for your specific dataset.

In the implementation example, we will demonstrate handling categorical features (if included in the dummy data) and briefly show an example of missing value imputation.

## 5. Implementation Example with Dummy Data

Let's implement Boosted Decision Trees using Python, utilizing `scikit-learn` and `xgboost`, with dummy data for both classification and regression tasks.

**Dummy Data Creation:**

We will generate synthetic datasets for binary classification and regression.

```python
import pandas as pd
import numpy as np
from sklearn.datasets import make_classification, make_regression
from sklearn.model_selection import train_test_split

# 1. Dummy Classification Dataset
X_clf, y_clf = make_classification(n_samples=1000, n_features=10, n_informative=8, n_classes=2, random_state=42)
X_train_clf, X_test_clf, y_train_clf, y_test_clf = train_test_split(X_clf, y_clf, test_size=0.3, random_state=42)

# 2. Dummy Regression Dataset
X_reg, y_reg = make_regression(n_samples=1000, n_features=10, noise=0.1, random_state=42)
X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X_reg, y_reg, test_size=0.3, random_state=42)

print("Classification Data Shapes:")
print("X_train_clf:", X_train_clf.shape, "y_train_clf:", y_train_clf.shape)
print("X_test_clf:", X_test_clf.shape, "y_test_clf:", y_test_clf.shape)
print("\nRegression Data Shapes:")
print("X_train_reg:", X_train_reg.shape, "y_train_reg:", y_train_reg.shape)
print("X_test_reg:", X_test_reg.shape, "y_test_reg:", y_test_reg.shape)
```

**Output:**

```
Classification Data Shapes:
X_train_clf: (700, 10) y_train_clf: (700,)
X_test_clf: (300, 10) y_test_clf: (300,)

Regression Data Shapes:
X_train_reg: (700, 10) y_train_reg: (700,)
X_test_reg: (300, 10) y_test_reg: (300,)
```

**Implementation using scikit-learn and xgboost:**

```python
from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor
from sklearn.metrics import accuracy_score, mean_squared_error, r2_score
import xgboost as xgb

# 1. GradientBoostingClassifier (scikit-learn)
gb_clf_sklearn = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)
gb_clf_sklearn.fit(X_train_clf, y_train_clf)
y_pred_clf_sklearn = gb_clf_sklearn.predict(X_test_clf)
accuracy_clf_sklearn = accuracy_score(y_test_clf, y_pred_clf_sklearn)
print("GradientBoostingClassifier (scikit-learn):")
print(f"  Accuracy: {accuracy_clf_sklearn:.4f}")

# 2. XGBoost Classifier
xgb_clf = xgb.XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, use_label_encoder=False, eval_metric='logloss', random_state=42) # use_label_encoder=False and eval_metric to avoid warnings
xgb_clf.fit(X_train_clf, y_train_clf)
y_pred_clf_xgb = xgb_clf.predict(X_test_clf)
accuracy_clf_xgb = accuracy_score(y_test_clf, y_pred_clf_xgb)
print("\nXGBoost Classifier:")
print(f"  Accuracy: {accuracy_clf_xgb:.4f}")

# 3. GradientBoostingRegressor (scikit-learn)
gb_reg_sklearn = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)
gb_reg_sklearn.fit(X_train_reg, y_train_reg)
y_pred_reg_sklearn = gb_reg_sklearn.predict(X_test_reg)
mse_reg_sklearn = mean_squared_error(y_test_reg, y_pred_reg_sklearn)
r2_reg_sklearn = r2_score(y_test_reg, y_pred_reg_sklearn)
print("\nGradientBoostingRegressor (scikit-learn):")
print(f"  Mean Squared Error (MSE): {mse_reg_sklearn:.4f}")
print(f"  R-squared (R2): {r2_reg_sklearn:.4f}")

# 4. XGBoost Regressor
xgb_reg = xgb.XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)
xgb_reg.fit(X_train_reg, y_train_reg)
y_pred_reg_xgb = xgb_reg.predict(X_test_reg)
mse_reg_xgb = mean_squared_error(y_test_reg, y_pred_reg_xgb)
r2_reg_xgb = r2_score(y_test_reg, y_pred_reg_xgb)
print("\nXGBoost Regressor:")
print(f"  Mean Squared Error (MSE): {mse_reg_xgb:.4f}")
print(f"  R-squared (R2): {r2_reg_xgb:.4f}")
```

**Output (Accuracy and scores might vary slightly):**

```
GradientBoostingClassifier (scikit-learn):
  Accuracy: 0.8867

XGBoost Classifier:
  Accuracy: 0.8867

GradientBoostingRegressor (scikit-learn):
  Mean Squared Error (MSE): 0.0115
  R-squared (R2): 0.9999

XGBoost Regressor:
  Mean Squared Error (MSE): 0.0112
  R-squared (R2): 0.9999
```

**Output Explanation:**

*   **Classification Accuracy:** For both `scikit-learn` and XGBoost classifiers, the accuracy is around 88.67%. This means the models correctly classified approximately 88.67% of the test samples. Accuracy is a measure of overall correctness in classification.
*   **Regression MSE (Mean Squared Error):**  MSE is very low (around 0.011-0.012) for both `scikit-learn` and XGBoost regressors, indicating small average squared errors between predicted and actual values. Lower MSE is better for regression models.
*   **Regression R-squared (R2):** R-squared is very close to 1 (0.9999) for both models. This signifies an excellent model fit, explaining almost all (99.99%) of the variance in the target variable. R2 of 1 represents a perfect fit.

**How to Read R-squared (R2) and MSE:**

*   **R-squared (R2):** (Explained in previous blogs, sections on accuracy metrics).  Value closer to 1 indicates a better fit for regression models.
*   **Mean Squared Error (MSE):** Represents the average squared difference between predictions and actual values. Lower MSE means better performance. MSE is in squared units of the target variable, so RMSE (Root Mean Squared Error, square root of MSE) is often used for easier interpretation as it is in the original unit.

In this dummy data example, both scikit-learn's Gradient Boosting implementations and XGBoost perform very well (high accuracy/R2, low MSE). This is expected with synthetic, relatively simple datasets. In real-world scenarios, you might observe more variance and different performance levels depending on the complexity of the data and the tuning of hyperparameters.

**Saving and Loading Models:**

```python
import joblib

# 1. Save scikit-learn GradientBoostingClassifier model
clf_sklearn_model_filename = 'gb_clf_sklearn_model.joblib'
joblib.dump(gb_clf_sklearn, clf_sklearn_model_filename)
print(f"scikit-learn GBC model saved to {clf_sklearn_model_filename}")

# Load scikit-learn GradientBoostingClassifier model
loaded_clf_sklearn_model = joblib.load(clf_sklearn_model_filename)
print("scikit-learn GBC model loaded.")
y_pred_loaded_clf_sklearn = loaded_clf_sklearn_model.predict(X_test_clf)
accuracy_loaded_clf_sklearn = accuracy_score(y_test_clf, y_pred_loaded_clf_sklearn)
print(f"Accuracy of loaded scikit-learn GBC model: {accuracy_loaded_clf_sklearn:.4f}")

# 2. Save XGBoost Classifier model
xgb_clf_model_filename = 'xgb_clf_model.json'
xgb_clf.save_model(xgb_clf_model_filename)
print(f"XGBoost Classifier model saved to {xgb_clf_model_filename}")

# Load XGBoost Classifier model
loaded_xgb_clf_model = xgb.XGBClassifier() # Need to initialize model object first
loaded_xgb_clf_model.load_model(xgb_clf_model_filename)
print("XGBoost Classifier model loaded.")
y_pred_loaded_xgb_clf = loaded_xgb_clf_model.predict(X_test_clf)
accuracy_loaded_xgb_clf = accuracy_score(y_test_clf, y_pred_loaded_xgb_clf)
print(f"Accuracy of loaded XGBoost Classifier model: {accuracy_loaded_xgb_clf:.4f}")

# 3. Save scikit-learn GradientBoostingRegressor model (similar saving/loading as classifier, just using regressor objects)
# ... (code for saving and loading scikit-learn GBR model, analogous to classifier example) ...

# 4. Save XGBoost Regressor model (similar to XGBoost classifier saving/loading)
xgb_reg_model_filename = 'xgb_reg_model.json'
xgb_reg.save_model(xgb_reg_model_filename)
print(f"XGBoost Regressor model saved to {xgb_reg_model_filename}")

loaded_xgb_reg_model = xgb.XGBRegressor()
loaded_xgb_reg_model.load_model(xgb_reg_model_filename)
print(f"XGBoost Regressor model loaded.")
y_pred_loaded_xgb_reg = loaded_xgb_reg_model.predict(X_test_reg)
mse_loaded_xgb_reg = mean_squared_error(y_test_reg, y_pred_loaded_xgb_reg)
print(f"MSE of loaded XGBoost Regressor model: {mse_loaded_xgb_reg:.4f}")
```

This code demonstrates saving and loading Boosted Decision Tree models (both classifiers and regressors, for both `scikit-learn` and XGBoost). Using `joblib` for `scikit-learn` and XGBoost's built-in `save_model` and `load_model` methods ensures that your trained models can be persisted and reused without retraining. The metrics after loading should be identical to the original trained models.

## 6. Post-processing

Post-processing for Boosted Decision Tree models typically involves model interpretation, feature importance analysis, and possibly evaluating model calibration or fairness, depending on the application.

**Feature Importance:**

Boosted Decision Trees (like Random Forests, Extra Trees) provide useful feature importance metrics that indicate how much each feature contributes to the model's predictive power.

*   **Methods for Feature Importance:**
    *   **Gini Importance (Mean Decrease Impurity) / Gain:** For tree-based models, this measures how much each feature contributes to reducing impurity (e.g., Gini index, entropy for classification; variance for regression) across all trees in the ensemble. Features that lead to larger impurity reduction are considered more important. `scikit-learn`'s `feature_importances_` attribute and XGBoost's `feature_importance_` method (with 'gain' type) provide this.
    *   **Permutation Importance:** Evaluates feature importance by measuring how much the model's performance decreases when a feature's values are randomly shuffled (permuted). Features that cause a larger performance drop when permuted are considered more important.  `scikit-learn`'s `permutation_importance` function can be used.

**Example of Feature Importance in Python (using scikit-learn and XGBoost):**

```python
import matplotlib.pyplot as plt

# 1. Feature Importance from scikit-learn GradientBoostingClassifier
feature_importance_sklearn_clf = gb_clf_sklearn.feature_importances_
feature_names_clf = [f'feature_{i+1}' for i in range(X_clf.shape[1])] # Example feature names

plt.figure(figsize=(10, 6))
plt.barh(feature_names_clf, feature_importance_sklearn_clf)
plt.xlabel('Feature Importance (Gini Importance)')
plt.ylabel('Features')
plt.title('Feature Importance from scikit-learn GradientBoostingClassifier')
plt.gca().invert_yaxis() # Invert y-axis to show most important at the top
plt.show()

# 2. Feature Importance from XGBoost Classifier
feature_importance_xgb_clf = xgb_clf.feature_importances_
plt.figure(figsize=(10, 6))
plt.barh(feature_names_clf, feature_importance_xgb_clf)
plt.xlabel('Feature Importance (Gain)') # XGBoost default is "gain" type
plt.ylabel('Features')
plt.title('Feature Importance from XGBoost Classifier')
plt.gca().invert_yaxis()
plt.show()

# ... (similar code for feature importance from GradientBoostingRegressor and XGBoostRegressor, replacing classifiers with regressors and adjusting plot titles) ...
```

**Interpretation of Feature Importance Plots:**

*   The horizontal bar charts visualize feature importance. Longer bars = higher importance.
*   Importance scores are usually normalized (sum to 1 or 100%). They show relative influence within the model, not necessarily absolute real-world importance.
*   Features with higher importance scores are considered more influential in the model's predictions. Lower scores suggest less influence. You might use feature importance for feature selection, data understanding, or model explanation.

**AB Testing and Hypothesis Testing (For Model Evaluation and Comparison):**

AB testing and hypothesis testing are primarily used for:

*   **Model Comparison:** Statistically compare performance of different Boosted Decision Tree models (e.g., with different hyperparameters, or different boosting libraries) or against other model types (like Random Forests, Logistic Regression) on a held-out test set or via cross-validation. Use hypothesis tests (t-tests, Wilcoxon tests, etc.) to determine if performance differences are statistically significant and not just random variation.
*   **Deployment Evaluation:** In real-world deployment, use AB testing to assess the impact of deploying a new Boosted Decision Tree model version compared to an existing version. Randomly assign users or traffic to groups (A and B), use different models for each group, and compare key performance metrics (conversion rate, click-through rate, error rates). Hypothesis testing can then evaluate if any observed differences are statistically significant.

**Other Post-processing Considerations:**

*   **Model Calibration:** For classification, check if the predicted probabilities are well-calibrated, meaning they accurately reflect the true likelihood of class membership. Calibration curves or reliability diagrams can be used to assess calibration, and calibration techniques can be applied if needed.
*   **Fairness and Bias Analysis:** In sensitive applications (e.g., loan approval, hiring), analyze the model for potential biases and fairness issues across different demographic groups. Metrics like disparate impact, equal opportunity, or demographic parity can be used to evaluate fairness. Post-processing techniques (or algorithmic adjustments) might be needed to mitigate bias if detected.

**In summary, post-processing for Boosted Decision Trees includes:**

*   Feature importance analysis to understand model behavior and feature relevance.
*   AB testing and hypothesis testing for rigorous model evaluation and comparison.
*   Potentially, model calibration and fairness/bias analysis, depending on the specific application context and requirements.

## 7. Tweakable Parameters and Hyperparameters

Boosted Decision Tree models are powerful, and their performance can be further optimized by tuning their hyperparameters. Let's explore the key hyperparameters for `GradientBoostingClassifier` and `GradientBoostingRegressor` in `scikit-learn` and equivalent parameters in `xgboost`.

**Key Hyperparameters (Common to `scikit-learn` and `xgboost`):**

1.  **`n_estimators` (or `num_boost_round` in `xgboost`):** Number of boosting stages or trees to build.
    *   **Effect:** Increasing `n_estimators` typically improves model performance, as it allows the model to learn more complex relationships. However, beyond a certain point, gains diminish, and it can lead to overfitting and longer training times.
    *   **Example Values:** `n_estimators=100`, `n_estimators=500`, `n_estimators=1000`, etc. Tune using cross-validation.
    *   **Tuning Strategy:** Start with a smaller value, gradually increase, monitor performance on validation data. Plotting validation error vs. `n_estimators` can help find the optimal point.

2.  **`learning_rate` (or `eta` in `xgboost`):** Scales the contribution of each tree. Also known as shrinkage.
    *   **Effect:** A smaller `learning_rate` makes the boosting process more conservative, requiring more trees (`n_estimators`) to achieve similar performance. It can often lead to better generalization and less overfitting, but training takes longer. A larger `learning_rate` speeds up training but might lead to overfitting.
    *   **Example Values:** `learning_rate=0.1`, `learning_rate=0.05`, `learning_rate=0.01`. Common to tune in conjunction with `n_estimators`. Smaller `learning_rate` often paired with larger `n_estimators`.
    *   **Tuning Strategy:**  Start with a moderate value (e.g., 0.1) and try smaller values, potentially increasing `n_estimators` accordingly.

3.  **`max_depth` (or `max_depth` in `xgboost`):** Maximum depth of individual decision trees (weak learners). Controls the complexity of each tree.
    *   **Effect:** Deeper trees are more complex and can capture more intricate relationships but are also more prone to overfitting. Shallower trees are simpler and less likely to overfit.
    *   **Example Values:** `max_depth=3`, `max_depth=5`, `max_depth=7`. Typical range for Gradient Boosting trees.
    *   **Tuning Strategy:** Try values in the range of 3-7 initially, tune using cross-validation.

4.  **`min_samples_split` (or `min_child_weight` in `xgboost`):** Minimum number of samples required to split an internal node in a tree. Controls tree complexity and overfitting. (`min_child_weight` in XGBoost is related but not directly the same, it's a minimum sum of instance weights needed in a child node, impacting tree pruning).
    *   **Effect:** Increasing `min_samples_split` makes trees more constrained and prevents overfitting.
    *   **Example Values:** `min_samples_split=2` (default), `min_samples_split=10`, `min_samples_split=20`. Higher values for larger datasets.

5.  **`min_samples_leaf` (or `min_child_samples` in `lightgbm`, also related to `min_child_weight` in `xgboost`):** Minimum number of samples required to be at a leaf node. Another way to control tree complexity and overfitting.
    *   **Effect:** Increasing `min_samples_leaf` also constrains trees and reduces overfitting.
    *   **Example Values:** `min_samples_leaf=1` (default), `min_samples_leaf=5`, `min_samples_leaf=10`.

6.  **`subsample` (or `subsample` in `xgboost`):** Fraction of training samples used for training each individual tree (stochastic gradient boosting).
    *   **Effect:** `subsample < 1` introduces randomness, reducing variance and overfitting. Also speeds up training.
    *   **Example Values:** `subsample=1.0` (no subsampling), `subsample=0.8`, `subsample=0.5`. Common range 0.5-1.0.
    *   **Tuning Strategy:** Try values like 0.8, 0.9, 1.0.

7.  **`colsample_bytree` (or `colsample_bytree` in `xgboost`, `feature_fraction` in `lightgbm`):** Fraction of features randomly sampled to consider for each tree.
    *   **Effect:** Introduces randomness in feature selection for each tree, helps reduce overfitting and can speed up training.
    *   **Example Values:** `colsample_bytree=1.0` (use all features), `colsample_bytree=0.8`, `colsample_bytree=0.5`.
    *   **Tuning Strategy:** Try values like 0.5, 0.7, 0.8, 1.0.

**Hyperparameter Tuning Example (using GridSearchCV for GradientBoostingClassifier in scikit-learn):**

```python
from sklearn.model_selection import GridSearchCV

param_grid_clf_tune = {
    'n_estimators': [100, 200, 300],
    'learning_rate': [0.01, 0.05, 0.1],
    'max_depth': [3, 4, 5],
    'min_samples_split': [2, 5],
    'min_samples_leaf': [1, 3],
    'subsample': [0.8, 1.0]
}

gb_classifier = GradientBoostingClassifier(random_state=42) # Base estimator
grid_search_clf_tune = GridSearchCV(estimator=gb_classifier, param_grid=param_grid_clf_tune,
                                     scoring='accuracy', cv=3, n_jobs=-1, verbose=2) # cv=3 for 3-fold CV, n_jobs for parallel
grid_search_clf_tune.fit(X_train_clf, y_train_clf)

best_gb_clf = grid_search_clf_tune.best_estimator_
best_params_clf_tune = grid_search_clf_tune.best_params_
best_score_clf_tune = grid_search_clf_tune.best_score_

print("\nBest Parameters for GradientBoostingClassifier from GridSearchCV:")
print(best_params_clf_tune)
print(f"Best Cross-validation Accuracy: {best_score_clf_tune:.4f}")

# Evaluate best model on test set
y_pred_best_gb_clf = best_gb_clf.predict(X_test_clf)
test_accuracy_best_gb_clf = accuracy_score(y_test_clf, y_pred_best_gb_clf)
print(f"Test Accuracy of Best GradientBoostingClassifier Model: {test_accuracy_best_gb_clf:.4f}")
```

**Explanation:**

*   `param_grid_clf_tune`: Defines the hyperparameter grid to search over for `GradientBoostingClassifier`.
*   `GridSearchCV`:  Systematically tries all hyperparameter combinations, evaluates using 3-fold cross-validation (`cv=3`), and selects the combination that maximizes 'accuracy'.
*   `scoring='accuracy'`: Metric used to evaluate and select best parameters.
*   `n_jobs=-1`: Use all available CPU cores for parallel processing to speed up tuning.
*   `grid_search_clf_tune.best_estimator_`: The best `GradientBoostingClassifier` model found.
*   `grid_search_clf_tune.best_params_`: The best hyperparameters.
*   `grid_search_clf_tune.best_score_`: Best cross-validation accuracy score.

Adapt `param_grid_clf_tune`, scoring metric, and cross-validation strategy as per your needs. Hyperparameter tuning is key to maximizing the performance of Boosted Decision Tree models and to prevent overfitting. You can use similar `GridSearchCV` setup for `GradientBoostingRegressor` and for other Gradient Boosting libraries like XGBoost and LightGBM (though hyperparameter names may slightly differ).

## 8. Accuracy Metrics

To assess the performance of Boosted Decision Tree models, we utilize appropriate accuracy metrics based on whether it's a classification or regression problem.

**Regression Metrics (for `GradientBoostingRegressor`):**

*   **Mean Squared Error (MSE):**  Average squared difference between predictions and actual values. Lower is better. Formula: $MSE = \frac{1}{n} \sum_{i=1}^{n} (\hat{y}_i - y_i)^2$.
*   **Root Mean Squared Error (RMSE):** Square root of MSE. In original units. Lower is better. Formula: $RMSE = \sqrt{MSE}$.
*   **Mean Absolute Error (MAE):** Average absolute difference. Less sensitive to outliers. Lower is better. Formula: $MAE = \frac{1}{n} \sum_{i=1}^{n} |\hat{y}_i - y_i|$.
*   **R-squared (Coefficient of Determination):** Proportion of variance explained. Ranges $-\infty$ to 1. Higher (closer to 1) is better. Formula: $R^2 = 1 - \frac{SS_{res}}{SS_{tot}}$.

**Classification Metrics (for `GradientBoostingClassifier`):**

*   **Accuracy:** Overall correctness. Higher is better. Formula: $Accuracy = \frac{TP + TN}{TP + TN + FP + FN}$.
*   **Precision:** Accuracy of positive predictions. Higher is better. Formula: $Precision = \frac{TP}{TP + FP}$.
*   **Recall (Sensitivity, True Positive Rate):** Coverage of actual positives. Higher is better. Formula: $Recall = \frac{TP}{TP + FN}$.
*   **F1-Score:** Harmonic mean of Precision and Recall. Balances both. Higher is better. Formula: $F1\text{-}Score = 2 \times \frac{Precision \times Recall}{Precision + Recall}$.
*   **AUC-ROC (Area Under ROC Curve):**  Ability to distinguish classes. Range 0 to 1. Higher is better.
*   **Confusion Matrix:** Table of TP, TN, FP, FN. Detailed performance view.

**Example in Python (Classification Metrics for `GradientBoostingClassifier`):**

```python
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# Assuming you have y_test_clf and y_pred_clf_sklearn from GradientBoostingClassifier

accuracy_clf = accuracy_score(y_test_clf, y_pred_clf_sklearn)
precision_clf = precision_score(y_test_clf, y_pred_clf_sklearn)
recall_clf = recall_score(y_test_clf, y_pred_clf_sklearn)
f1_clf = f1_score(y_test_clf, y_pred_clf_sklearn)
auc_roc_clf = roc_auc_score(y_test_clf, y_pred_clf_sklearn)
conf_matrix_clf = confusion_matrix(y_test_clf, y_pred_clf_sklearn)

print("GradientBoostingClassifier Evaluation Metrics:")
print(f"  Accuracy: {accuracy_clf:.4f}")
print(f"  Precision: {precision_clf:.4f}")
print(f"  Recall: {recall_clf:.4f}")
print(f"  F1-Score: {f1_clf:.4f}")
print(f"  AUC-ROC: {auc_roc_clf:.4f}")

print("\nConfusion Matrix for GradientBoostingClassifier:")
print(conf_matrix_clf)

# Optional: Visualize Confusion Matrix using Seaborn and Matplotlib
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix_clf, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Predicted 0', 'Predicted 1'],
            yticklabels=['Actual 0', 'Actual 1'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix for GradientBoostingClassifier')
plt.show() # Display if in environment that supports plots
```

Choose metrics that best reflect your problem's objectives and data characteristics. For regression, MSE, RMSE, MAE, R2 are standard. For classification, accuracy, precision, recall, F1-score, AUC-ROC, and the confusion matrix provide comprehensive performance insights.

## 9. Model Productionizing Steps

Productionizing Boosted Decision Tree models follows general ML model deployment practices. Key steps are similar to those for Stacking and Extra Trees, with some specifics.

**General Productionization Steps:**

1.  **Training and Selection:** Train, tune, and select your optimal Boosted Decision Tree model (classifier or regressor).
2.  **Model Saving:** Save the trained model using `joblib` (for scikit-learn) or model-specific save methods (e.g., `xgboost.save_model`).
3.  **Environment Setup:** Choose deployment environment: Cloud (AWS, Google Cloud, Azure), On-Premises, Local.
4.  **API Development:** Create an API (e.g., Flask, FastAPI in Python) to serve predictions.
5.  **Deployment:** Deploy API and model.
6.  **Testing and Monitoring:** Rigorous testing, monitoring performance, errors, etc.

**Code Example: Flask API for XGBoost Classifier (adapted for Boosted DTs):**

```python
from flask import Flask, request, jsonify
import xgboost as xgb
import numpy as np

app = Flask(__name__)

# Load trained XGBoost Classifier model
model_filename = 'xgb_clf_model.json' # File path for saved XGBoost model
loaded_xgb_model = xgb.XGBClassifier()
loaded_xgb_model.load_model(model_filename)

@app.route('/predict', methods=['POST'])
def predict():
    try:
        data = request.get_json()
        features = data['features'] # Assuming input features in JSON
        input_data = np.array(features).reshape(1, -1)

        prediction = loaded_xgb_model.predict(input_data).tolist() # Get prediction as list

        return jsonify({'prediction': prediction}) # Return prediction in JSON format
    except Exception as e:
        return jsonify({'error': str(e)}), 400 # Handle errors, return error message

if __name__ == '__main__':
    app.run(debug=True) # debug=True for development; set to False for production
```

**Productionization Considerations for Boosted Decision Trees:**

*   **Prediction Latency:** Boosted Decision Trees can be slightly slower in prediction than simpler models (e.g., linear models, single decision trees), especially with large `n_estimators`. Optimize hyperparameters for latency if real-time prediction is crucial. Libraries like LightGBM and XGBoost are generally very efficient.
*   **Model Size:**  Models can be moderately large, especially with many trees. Consider model compression techniques if file size or memory usage is a concern.
*   **Resource Consumption:** Can be more resource-intensive during training and prediction compared to simpler models. Monitor resource usage in deployment environment and choose appropriate infrastructure.
*   **Model Updates:** Plan for periodic model retraining with new data to maintain accuracy over time. Implement automated retraining pipelines.
*   **Monitoring is Crucial:** Implement robust monitoring for API health, prediction latency, error rates, and model performance metrics in production to detect issues and model drift.
*   **Version Control and CI/CD:** Use version control for all code and model files. Set up CI/CD for automated building, testing, and deployment of model updates.

The provided Flask API is a starting point for local testing. For cloud or on-premises deployment, consider containerization (Docker), cloud-managed ML services (AWS SageMaker, Google Vertex AI, Azure ML), API gateways, load balancing, and comprehensive monitoring and logging systems for production robustness and scalability.

## 10. Conclusion: Boosted Decision Trees - A Robust and Versatile Algorithm

Boosted Decision Trees are a powerful and widely applicable machine learning algorithm. Their ability to combine simple decision trees into a strong, accurate ensemble, coupled with their robustness and flexibility, makes them a valuable tool in many domains.

**Real-World Impact and Continued Relevance:**

Boosted Decision Trees are extensively used across various industries:

*   **Finance:** Fraud detection, credit risk scoring, algorithmic trading, customer churn prediction.
*   **Healthcare:** Disease diagnosis and prediction, patient risk stratification, drug discovery.
*   **E-commerce and Marketing:** Recommendation systems, personalized marketing, customer segmentation, ad click prediction, sales forecasting.
*   **Natural Language Processing (NLP):** Text classification, sentiment analysis, information extraction.
*   **Logistics and Operations:** Demand forecasting, supply chain optimization, predictive maintenance.

**Why Boosted Decision Trees Remain Popular:**

*   **High Predictive Accuracy:** Often achieve state-of-the-art or near state-of-the-art performance on many structured/tabular data problems.
*   **Versatility:** Can handle both regression and classification tasks, and work effectively with mixed data types (numerical, categorical).
*   **Robustness:** Relatively robust to outliers and noisy data compared to some algorithms.
*   **Interpretability (Feature Importance):** Feature importance metrics provide insights into which features are most influential in the model's decisions, enhancing understanding and explainability.
*   **Optimized Implementations:** Libraries like XGBoost, LightGBM, and CatBoost provide highly efficient, fast, and scalable implementations, making Boosted Decision Trees practical for large datasets.

**Optimized and Newer Algorithms:**

While Boosted Decision Trees are excellent, the field of machine learning is always evolving. Some related and newer algorithms include:

*   **Gradient Boosting Enhancements (XGBoost, LightGBM, CatBoost):** These are in themselves optimized and advanced forms of Gradient Boosting, often outperforming basic Gradient Boosting in speed, efficiency, and sometimes accuracy. They are highly recommended in practice.
*   **Deep Learning:** For certain types of problems, especially in image recognition, NLP, and speech processing, deep neural networks have surpassed traditional machine learning methods in performance. However, for many structured data tasks, Boosted Decision Trees (especially optimized versions) remain highly competitive and often more efficient to train.
*   **Tree-based Deep Learning Hybrids:** Research continues to explore combinations of tree-based models with neural network architectures, seeking to blend the strengths of both approaches.
*   **AutoML (Automated Machine Learning) Tools:** Many AutoML platforms automatically incorporate Boosted Decision Trees (and related methods) in their model selection and ensembling processes, streamlining the process of building high-performance models.

**Conclusion:**

Boosted Decision Trees, and particularly their advanced implementations like XGBoost, LightGBM, and CatBoost, are essential algorithms in the machine learning toolkit. Their consistent high performance, versatility, and interpretability ensure their continued relevance and wide application in solving a vast array of real-world problems. They are a powerful and reliable choice for tackling structured data tasks, and every data scientist and machine learning practitioner should have a solid understanding of Boosted Decision Trees and their practical application.

## 11. References

1.  **Friedman, J. H. (2001). Greedy function approximation: A gradient boosting machine.** *Machine learning*, *38*, 119-141. [https://doi.org/10.1023/A:1009945113576](https://doi.org/10.1023/A:1009945113576) - *The foundational paper introducing Gradient Boosting Machines.*
2.  **Hastie, T., Tibshirani, R., & Friedman, J. (2009). *The elements of statistical learning: data mining, inference, and prediction*. Springer Science & Business Media.** - *A comprehensive textbook covering statistical learning, including detailed explanations of boosting methods.* [Freely available online version: https://web.stanford.edu/~hastie/ElemStatLearn/](https://web.stanford.edu/~hastie/ElemStatLearn/)
3.  **Chen, T., & Guestrin, C. (2016). XGBoost: A scalable tree boosting system.** In *Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining* (pp. 785-794). ACM. [https://doi.org/10.1145/2939672.2939785](https://doi.org/10.1145/2939672.2939785) - *Paper introducing XGBoost, a highly efficient Gradient Boosting implementation.*
