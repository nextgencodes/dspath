---
title: "Unleashing Creativity with Generative Adversarial Networks (GANs): A Beginner's Guide"
excerpt: "Generative Adversarial Networks (GANs) Algorithm"
# permalink: /courses/nn/gan/
last_modified_at: 2022-01-22T23:45:00-00:00
classes: narrow
hidden: false
strip_title: true
categories: 
  - Neural Network
  - Deep Neural Network
  - Generative Model
  - Unsupervised Learning
  - Generative Learning
tags: 
  - Neural Networks
  - Deep Learning
  - Generative models
  - Adversarial learning
---

{% include download file="gans_blog_code.ipynb" alt="download GANs code" text="Download Code" %}

## The Art of Imitation: Introducing Generative Adversarial Networks

Imagine you have two artists. One artist, let's call them the **Forger (Generator)**, tries to create fake paintings that look as realistic as possible, like imitations of famous masterpieces. The other artist, the **Art Critic (Discriminator)**, is an expert in art and tries to distinguish between real paintings and the forgeries.

This is the core idea behind **Generative Adversarial Networks (GANs)**. GANs are a fascinating type of machine learning algorithm that learn to generate new data that is similar to some training data.  They do this through a clever game between two neural networks: the **Generator** and the **Discriminator**, constantly competing with each other.

The Generator's goal is to create fake data (like images, text, or music) that is so realistic that the Discriminator cannot tell it apart from real data. The Discriminator's goal is to become better and better at distinguishing between real and fake data.  As these two networks train in an adversarial manner (playing against each other), both become stronger, leading to the Generator producing increasingly realistic data.

**Real-World Examples of GANs in Action:**

*   **Generating Realistic Images:** GANs are famous for their ability to generate incredibly realistic images of faces, animals, landscapes, and objects. You might have seen examples of "AI-generated faces" online â€“ these are often created using GANs. Think about creating new styles of shoes or furniture designs automatically.
*   **Image Editing and Manipulation:** GANs can be used for tasks like turning black and white photos into color photos, increasing the resolution of blurry images (super-resolution), or even editing images in creative ways, like changing the style of a photograph to look like a painting by Van Gogh.
*   **Creating Synthetic Data for Training:** In some situations, we might not have enough real data to train machine learning models effectively. GANs can be used to generate synthetic but realistic data that can be used to augment (increase) the training dataset, improving the performance of other models. For example, generating more medical images to train a model to detect diseases.
*   **Generating Music and Art:** GANs aren't limited to images. They can also generate music, text, and even 3D models. Think about AI creating new melodies or writing poems in a particular style.
*   **Drug Discovery and Molecule Generation:** GANs are being explored in the field of drug discovery to generate novel molecules with desired properties, which can speed up the process of finding new medicines.

In essence, GANs are powerful tools for learning to create new things by learning from existing examples, pushing the boundaries of AI creativity.

## The Mathematical Duel: How GANs Learn

Let's delve into the mathematics that powers the adversarial game in GANs.  We'll break down the core components: the Generator, the Discriminator, and the loss functions that drive their competition.

**The Players:**

*   **Generator (G):**  The Generator takes random noise as input (think of it as a starting point for creativity) and transforms it into data that should resemble the real data. We can represent the noise as a random vector **z** and the Generator as a function **G(z)**.  Ideally, **G(z)** will produce fake data that is indistinguishable from real data.

*   **Discriminator (D):** The Discriminator receives data as input, and its job is to classify whether the input is real (from the training dataset) or fake (generated by the Generator).  We can represent the Discriminator as a function **D(x)**, where **x** is the input data. **D(x)** should output a probability, ideally close to 1 if **x** is real and close to 0 if **x** is fake.

**The Game: A Minimax Objective**

GANs are trained using a **minimax game**. The Discriminator tries to *maximize* its ability to distinguish between real and fake data, while the Generator tries to *minimize* the Discriminator's ability to do so.  This game is formalized using a **value function** or **loss function** called the **Adversarial Loss**.

Let's represent the adversarial loss, often called the **GAN loss** or **value function V(D, G)**, as:

$$ \min_G \max_D V(D, G) = \mathbb{E}_{x \sim p_{data}(x)} [\log D(x)] + \mathbb{E}_{z \sim p_{z}(z)} [\log (1 - D(G(z)))] $$

Let's break down this equation step-by-step:

*   **$\min_G \max_D V(D, G)$:** This part indicates that we want to find a Generator **G** that *minimizes* the value function, while simultaneously finding a Discriminator **D** that *maximizes* it. This represents the adversarial nature of the game.

*   **$V(D, G) = \mathbb{E}_{x \sim p_{data}(x)} [\log D(x)] + \mathbb{E}_{z \sim p_{z}(z)} [\log (1 - D(G(z)))]$:** This is the value function itself, consisting of two parts:

    1.  **$\mathbb{E}_{x \sim p_{data}(x)} [\log D(x)]$: Discriminator's Success on Real Data:**
        *   $\mathbb{E}$ denotes the **expected value** (average).
        *   $x \sim p_{data}(x)$ means we are drawing real data samples **x** from the **real data distribution** $p_{data}(x)$.
        *   $D(x)$ is the Discriminator's output (probability) for a real data sample **x**.
        *   $\log D(x)$ is the logarithm of the Discriminator's output. Since $D(x)$ should be close to 1 for real data, $\log D(x)$ will be close to 0 (because $\log(1)=0$ and $\log(x)$ increases as $x$ approaches 1). For values of $D(x)$ closer to 0 (incorrectly classified real samples), $\log D(x)$ becomes a large negative number.
        *   **The Discriminator wants to *maximize* this term.** By maximizing $\mathbb{E}_{x \sim p_{data}(x)} [\log D(x)]$, the Discriminator is encouraged to correctly classify real data as real, making $D(x)$ close to 1, and thus making $\log D(x)$ as close to 0 (or as large a negative number as possible, effectively maximizing towards 0 in the log space).

    2.  **$\mathbb{E}_{z \sim p_{z}(z)} [\log (1 - D(G(z)))]$: Discriminator's Failure on Fake Data (Generator's Success):**
        *   $\mathbb{E}$ denotes the expected value.
        *   $z \sim p_{z}(z)$ means we are drawing random noise samples **z** from a noise distribution $p_{z}(z)$ (e.g., a Gaussian distribution).
        *   $G(z)$ is the data generated by the Generator from noise **z** (fake data).
        *   $D(G(z))$ is the Discriminator's output (probability) for the fake data **G(z)**.
        *   $1 - D(G(z))$ is one minus the Discriminator's output for fake data.  If the Generator is doing well, it will create fake data that looks real, and the Discriminator will be tricked into outputting a probability close to 1 for $D(G(z))$. In this case, $1 - D(G(z))$ will be close to 0, and $\log (1 - D(G(z)))$ will be a large negative number. If the Generator is doing poorly and the Discriminator can easily identify fake data (outputting $D(G(z))$ close to 0), then $1 - D(G(z))$ will be close to 1, and $\log (1 - D(G(z)))$ will be close to 0.
        *   **The Generator wants to *maximize* this term (or equivalently, minimize the negative of this term).**  However, in the minimax formulation, we consider the *Discriminator's* perspective of maximization. The Discriminator wants to *minimize* this term. Why?  Because if the Discriminator is good at identifying fake data, it will output $D(G(z))$ close to 0 for fake data, making $1 - D(G(z))$ close to 1 and $\log(1 - D(G(z)))$ close to 0.  If the Discriminator is fooled by the Generator's fake data (outputs $D(G(z))$ close to 1), then $1 - D(G(z))$ becomes close to 0, and $\log(1 - D(G(z)))$ becomes a large negative number. Thus, by maximizing the *entire* value function V(D,G), the Discriminator is indirectly minimizing the second term, meaning it wants to be able to confidently identify fake data as fake (outputting $D(G(z))$ close to 0).

        *   **From the Generator's perspective (minimization):** The Generator wants to minimize the entire value function. To do this, the Generator focuses on the second term: $\mathbb{E}_{z \sim p_{z}(z)} [\log (1 - D(G(z)))]$. To minimize this, the Generator needs to generate data $G(z)$ that fools the Discriminator, making $D(G(z))$ close to 1. If $D(G(z))$ is close to 1, then $1 - D(G(z))$ is close to 0, and $\log(1 - D(G(z)))$ becomes a large negative number, thus minimizing this term.

**In simpler terms:**

*   **Discriminator's Goal:** Maximize $\log D(x) + \log(1 - D(G(z)))$.  It wants to maximize $\log D(x)$ by correctly identifying real images and maximize $\log(1-D(G(z)))$ by correctly identifying fake images.
*   **Generator's Goal:** Minimize $\log (1 - D(G(z)))$ (or maximize $\log D(G(z))$ in practice for better gradient flow - a common trick). It wants to fool the Discriminator, so $D(G(z))$ should be close to 1 for fake images, thus making $\log (1 - D(G(z)))$ as small (negative) as possible.

**Training Process - Iterative Improvement:**

1.  **Discriminator Training:** In each iteration, we train the Discriminator to better distinguish real data from the current fake data generated by the Generator. We keep the Generator's weights fixed during this step and update only the Discriminator's weights to maximize the value function (or minimize the Discriminator loss, which is essentially the negative of the value function from the Discriminator's perspective).

2.  **Generator Training:** Then, we train the Generator to produce better fake data that can fool the current Discriminator. We keep the Discriminator's weights fixed and update only the Generator's weights to minimize the value function (or minimize the Generator loss, which is designed to encourage the Generator to produce data that the Discriminator classifies as real).

These two steps are repeated iteratively. As the training progresses, the Discriminator becomes better at distinguishing real from fake, and simultaneously, the Generator becomes better at creating realistic fake data. Ideally, this process continues until the Generator produces data that is so realistic that the Discriminator can no longer reliably distinguish it from real data (i.e., $D(G(z))$ approaches 0.5, meaning the Discriminator is essentially guessing).

## Prerequisites and Preprocessing for GANs

Before you start building and training GANs, let's consider the prerequisites and data preparation steps.

**Prerequisites and Assumptions:**

1.  **Sufficient Training Data:** GANs, especially for complex tasks like image generation, typically require a large amount of high-quality training data. The GAN learns to mimic the patterns and characteristics of the data it is trained on. If your training data is limited or not representative of the data you want to generate, the GAN's performance will likely be poor.
2.  **Data Distribution Understanding (Implicit):** GANs implicitly learn the underlying distribution of your training data. The assumption is that there *is* an underlying structure or pattern in your data that the GAN can learn to generate similar data from.
3.  **Computational Resources (Especially for Complex Data):** Training GANs, particularly for high-resolution images or complex data types, can be computationally intensive and time-consuming. You will likely need access to GPUs to train GANs effectively in a reasonable timeframe.
4.  **Understanding of Neural Networks (Essential):** A good understanding of neural networks, including concepts like layers, activation functions, loss functions, optimizers, and backpropagation, is essential for working with GANs. GANs are composed of neural networks for both the Generator and the Discriminator.

**Testing the Assumptions (Practical Considerations):**

*   **Data Quality Check:** Examine your training data. Is it clean, relevant, and representative of what you want to generate? Poor data in will lead to poor data out.
*   **Baseline Model (Optional but Recommended):** Before diving into GANs, it can be helpful to train a simpler generative model (like an autoencoder or a Variational Autoencoder - VAE) on your data. If even simpler models struggle to learn meaningful representations, it might indicate issues with data quality or complexity that could also affect GAN training.
*   **Start Simple:** Begin with simpler GAN architectures and datasets to understand the training dynamics and challenges. Gradually increase complexity as you gain experience.

**Required Python Libraries:**

*   **Deep Learning Framework:**
    *   **TensorFlow/Keras:** Very popular for GAN implementation due to its flexibility and Keras's high-level API. TensorFlow 2.x and Keras are well-suited for GANs.
    *   **PyTorch:** Another widely used framework, known for its dynamic computation graph and research-friendliness. PyTorch is also excellent for GANs.

*   **Numerical Computation and Data Handling:**
    *   **NumPy:** For numerical operations, especially with arrays.
    *   **Pandas:** For data manipulation (if your data is in tabular form initially, though GANs are often used with image data).

*   **Image Processing (if working with images):**
    *   **PIL (Pillow):** Python Imaging Library for image loading, saving, and basic manipulation.
    *   **OpenCV (cv2):** For more advanced image processing tasks, though PIL is often sufficient for basic GAN examples.

*   **Visualization:**
    *   **Matplotlib:** For plotting and visualizing generated samples, training progress (loss curves), etc.

You can install these libraries using pip:

```bash
pip install tensorflow numpy pandas pillow matplotlib  # For TensorFlow
# or
pip install torch torchvision numpy pandas pillow matplotlib # For PyTorch (torchvision for datasets and image transformations)
```

## Data Preprocessing for GANs:  The Importance of Normalization

Data preprocessing is almost always crucial for successful GAN training, especially when working with continuous data like images.  **Normalization** is a key preprocessing step for GANs.

**Why Data Preprocessing (Normalization) is Important for GANs:**

1.  **Improved Training Stability:** GAN training is notoriously unstable. Normalizing input data helps stabilize the training process and can prevent issues like mode collapse (where the Generator produces only a limited variety of outputs) or vanishing/exploding gradients.
2.  **Activation Function Ranges:** Many GAN architectures use activation functions like `tanh` (range -1 to 1) or `sigmoid` (range 0 to 1) in the output layer of the Generator to constrain the generated data to a specific range (e.g., pixel values for images). Normalizing the real training data to a similar range ensures consistency between the real and generated data in terms of scale.
3.  **Discriminator Performance:** If the real data and fake data have vastly different scales, it can make it too easy for the Discriminator to distinguish them, especially in early training stages. Normalization helps to level the playing field and forces the Discriminator to learn more subtle features instead of just relying on scale differences.

**Common Normalization Techniques for GANs (Especially for Images):**

*   **Pixel Value Scaling to [-1, 1] or [0, 1]:**

    *   **Scaling to [-1, 1]:**  Often used when the Generator's output layer uses `tanh` activation. If original pixel values are in the range [0, 255], you can scale them to [-1, 1] using:
        $$ X_{scaled} = \frac{X}{127.5} - 1 $$
        where X is the original pixel value.

    *   **Scaling to [0, 1]:** Used when the Generator's output layer uses `sigmoid` activation (or no activation with appropriate loss functions). If original pixel values are in [0, 255], you can scale to [0, 1] using:
        $$ X_{scaled} = \frac{X}{255.0} $$

    *   **Example (scaling to [-1, 1] in Python):**
        ```python
        import numpy as np
        from PIL import Image

        # Load image using Pillow (assuming pixel values 0-255)
        image = Image.open("your_image.jpg")
        image_array = np.array(image).astype(np.float32) # Convert to float32 for division

        # Normalize to [-1, 1]
        normalized_image = (image_array / 127.5) - 1.0

        print("Original pixel range:", image_array.min(), image_array.max()) # Output: typically 0.0 255.0
        print("Normalized pixel range:", normalized_image.min(), normalized_image.max()) # Output: should be approximately -1.0 1.0
        ```

*   **Channel-wise Normalization (Less Common but sometimes used):** Involves normalizing each color channel (Red, Green, Blue for RGB images) separately.  You might calculate the mean and standard deviation for each channel across your training dataset and then standardize each channel using these statistics.  This is similar to standardization (Z-score scaling) but applied channel-wise.

**When Can You Potentially Ignore Preprocessing (Normalization)?**

*   **Binary Data (Maybe):** If you are working with binary data (e.g., generating black and white images where pixel values are strictly 0 or 1), normalization might be less critical in terms of scale, but it might still be beneficial for training stability depending on the GAN architecture and loss functions used.
*   **Very Simple Data Distributions (Toy Examples):** For very simple toy datasets or examples used for learning concepts, you might sometimes get away without explicit normalization, but it's generally a good practice to normalize your data, especially for real-world applications and more complex data types.
*   **Specific GAN Architectures/Loss Functions (Advanced):** Some advanced GAN architectures or loss functions might be designed to be less sensitive to data scaling issues, but for standard GANs like DCGAN, WGAN, etc., normalization is usually recommended.

**Important Considerations for Image Data:**

*   **Consistency:** Apply the *same* normalization method to both your training data and any data you want to generate from the GAN. If you normalize your training images to [-1, 1], make sure to denormalize the generated images back to the original pixel value range (e.g., [0, 255]) for display or further use.
*   **Data Augmentation (Often Used):** In addition to normalization, data augmentation techniques (like random cropping, flipping, rotations) are commonly used when training GANs for image generation. Data augmentation helps to increase the diversity of the training data and improve the GAN's generalization ability.

In summary, for most GAN applications, especially with image data, normalizing your input data (e.g., scaling pixel values to [-1, 1] or [0, 1]) is a crucial preprocessing step that can significantly improve training stability, convergence, and the quality of generated samples.

## Implementing a Simple GAN with Dummy Data: Hands-on Example

Let's implement a basic GAN using Keras (TensorFlow) and dummy data. We'll create a simple GAN to generate 2D points that resemble a circle distribution.

**1. Generate Dummy 2D Data (Circle Distribution):**

```python
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf # or from tensorflow import keras

# Function to generate points in a circle
def generate_real_samples(n_samples):
    radius = 5
    angles = np.random.uniform(0, 2*np.pi, n_samples)
    radii = np.random.normal(radius, 0.5, n_samples) # Add some noise to radius
    x = radii * np.cos(angles)
    y = radii * np.sin(angles)
    points = np.column_stack((x, y))
    return points

# Generate and plot real samples
real_data = generate_real_samples(1000)
plt.scatter(real_data[:, 0], real_data[:, 1], s=10, label='Real Data')
plt.title('Real Data Distribution (Circle)')
plt.xlabel('x')
plt.ylabel('y')
plt.legend()
plt.axis('equal') # Make axes scaled equally
plt.show()
```

This code will generate and plot points distributed roughly in a circle.

**2. Define the Generator Model:**

```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

def define_generator(latent_dim):
    model = Sequential(name='generator')
    model.add(Dense(128, activation='relu', input_dim=latent_dim))
    model.add(Dense(128, activation='relu'))
    model.add(Dense(2, activation='linear')) # Output layer with 2 neurons for 2D points
    return model

latent_dim = 10 # Dimensionality of the random noise vector (z)
generator = define_generator(latent_dim)
generator.summary()
```

This defines a simple Generator network that takes a `latent_dim` dimensional noise vector as input and outputs 2D points.

**3. Define the Discriminator Model:**

```python
def define_discriminator():
    model = Sequential(name='discriminator')
    model.add(Dense(128, activation='relu', input_dim=2)) # Input is 2D point
    model.add(Dense(128, activation='relu'))
    model.add(Dense(1, activation='sigmoid')) # Output layer - probability (real or fake)
    return model

discriminator = define_discriminator()
discriminator.summary()
```

This defines a Discriminator network that takes a 2D point as input and outputs a probability of it being real.

**4. Define the GAN Model (Combining Generator and Discriminator):**

```python
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam

def define_gan(generator, discriminator):
    # Make discriminator NOT trainable when training the generator in GAN context
    discriminator.trainable = False # Crucial step!
    gan_output = discriminator(generator.output) # Connect Discriminator to Generator's output
    model = Model(inputs=generator.input, outputs=gan_output)
    optimizer = Adam(learning_rate=0.0002, beta_1=0.5) # Common GAN optimizer settings
    model.compile(loss='binary_crossentropy', optimizer=optimizer) # Binary crossentropy for GAN loss
    return model

gan_model = define_gan(generator, discriminator)
gan_model.summary()
```

This creates the combined GAN model.  Note that we set `discriminator.trainable = False` when training the GAN model itself. This is because when we train the *Generator*, we want to update only the Generator's weights, keeping the Discriminator's weights fixed, as we are trying to fool the *current* Discriminator.

**5. Training the GAN:**

```python
def train_gan(generator, discriminator, gan_model, latent_dim, n_epochs=5000, batch_size=128):
    batch_per_epoch = real_data.shape[0] // batch_size
    half_batch = batch_size // 2

    history = {'d_loss_real': [], 'd_loss_fake': [], 'g_loss': []} # To track losses

    for epoch in range(n_epochs):
        for batch_idx in range(batch_per_epoch):
            # 1. Train Discriminator
            # Select a batch of real samples
            idx = np.random.randint(0, real_data.shape[0], half_batch)
            real_samples = real_data[idx]
            # Generate a batch of fake samples
            noise = np.random.randn(half_batch, latent_dim)
            fake_samples = generator.predict(noise)

            # Train discriminator on real and fake samples separately
            discriminator_loss_real = discriminator.train_on_batch(real_samples, np.ones((half_batch, 1))) # Labels for real samples are 1
            discriminator_loss_fake = discriminator.train_on_batch(fake_samples, np.zeros((half_batch, 1))) # Labels for fake samples are 0
            discriminator_loss = 0.5 * np.add(discriminator_loss_real, discriminator_loss_fake) # Average loss

            # 2. Train Generator
            noise = np.random.randn(batch_size, latent_dim)
            generator_loss = gan_model.train_on_batch(noise, np.ones((batch_size, 1))) # Labels for fake samples are 1 (fool discriminator)

            # Record losses
            history['d_loss_real'].append(discriminator_loss_real)
            history['d_loss_fake'].append(discriminator_loss_fake)
            history['g_loss'].append(generator_loss)

        # Print progress every few epochs
        if (epoch + 1) % 200 == 0:
            print(f'Epoch {epoch+1}/{n_epochs}, D Real Loss={discriminator_loss_real:.3f}, D Fake Loss={discriminator_loss_fake:.3f}, G Loss={generator_loss:.3f}')

    return history

# Train the GAN
history = train_gan(generator, discriminator, gan_model, latent_dim)

# Plot training losses
plt.figure(figsize=(10, 5))
plt.plot(history['d_loss_real'], label='Discriminator Loss (Real)')
plt.plot(history['d_loss_fake'], label='Discriminator Loss (Fake)')
plt.plot(history['g_loss'], label='Generator Loss')
plt.title('GAN Training Losses')
plt.xlabel('Iterations')
plt.ylabel('Loss')
plt.legend()
plt.show()
```

This `train_gan` function implements the iterative training process: training the Discriminator and then the Generator in each iteration. We track and plot the losses to monitor training progress.

**6. Generate Samples and Visualize:**

```python
# Generate samples using the trained generator
n_generated_samples = 1000
noise = np.random.randn(n_generated_samples, latent_dim)
generated_data = generator.predict(noise)

# Plot generated samples vs real data
plt.figure(figsize=(8, 6))
plt.scatter(real_data[:, 0], real_data[:, 1], s=10, label='Real Data', alpha=0.5) # Reduced alpha for better visibility
plt.scatter(generated_data[:, 0], generated_data[:, 1], s=10, label='Generated Data', alpha=0.5, color='red')
plt.title('Real vs Generated Data Distribution')
plt.xlabel('x')
plt.ylabel('y')
plt.legend()
plt.axis('equal')
plt.show()
```

This code generates new 2D points using the trained Generator and plots them alongside the real data to visually compare the distributions. Ideally, the generated points should also form a circular shape, similar to the real data.

**Output Explanation:**

*   **Loss Curves:** The loss curves (Discriminator Real Loss, Discriminator Fake Loss, Generator Loss) during training can give insights into the GAN's training dynamics. Ideally, the Discriminator losses should decrease (meaning it's getting better at distinguishing real from fake), and the Generator loss should also decrease (or fluctuate but not increase indefinitely), indicating the Generator is becoming more successful at fooling the Discriminator. Unstable loss curves are common in GAN training and can be challenging to interpret.
*   **Visual Comparison of Distributions:** The scatter plot of real vs. generated data visually shows how well the Generator has learned to mimic the real data distribution. In this example, you should expect to see the generated points also forming a circle-like shape, if training is successful. If the generated points are scattered randomly or do not resemble a circle, it might indicate issues with training, hyperparameters, or model architecture.

**Saving and Loading Generator (for later use):**

```python
# Save the generator model
generator.save('generator_model') # Saves to a directory 'generator_model'

# Load the generator model later
from tensorflow.keras.models import load_model
loaded_generator = load_model('generator_model')

# Generate samples with the loaded generator (verify loading)
loaded_generated_data = loaded_generator.predict(noise)
# ... (you can plot and compare loaded_generated_data to previous generated data)
```

We only need to save and load the Generator, as that's the model we use for generating new data after training.

This example provides a basic hands-on implementation of a GAN for generating simple 2D data. For more complex data like images, you'll need to use more sophisticated architectures (like Convolutional GANs - DCGANs) and datasets.

## Post-Processing and Analysis of GANs

Post-processing for GANs often focuses on evaluating and understanding the quality and diversity of the generated data, as well as identifying and mitigating common problems like mode collapse. Unlike discriminative models, there isn't a direct measure of "feature importance" in the traditional sense for GANs.

**1. Visual Inspection of Generated Samples:**

*   **Manual Evaluation (Qualitative):** For image GANs, the most common and often most informative post-processing step is to simply *look* at the generated images.  Visually assess:
    *   **Realism:** Do the generated samples look realistic and similar to real images from the training data?
    *   **Diversity:** Are the generated samples diverse, or do they all look very similar (mode collapse)?
    *   **Artifacts:** Are there noticeable artifacts or flaws in the generated images (e.g., blurring, strange patterns)?

*   **Grid of Samples:** Generate a grid of samples from different random noise vectors and visualize them together. This helps to get an overview of the variety and quality of generated outputs.

**2. Quantitative Evaluation Metrics (for more objective assessment):**

While visual inspection is crucial, quantitative metrics can provide more objective ways to compare different GAN models or track training progress. However, GAN evaluation metrics are still an active area of research and have limitations.

*   **Inception Score (IS):**

    *   **Concept:**  Uses a pre-trained Inception image classification model. For each generated image, calculate the class probabilities predicted by Inception. A good GAN should generate images that are:
        1.  **Classifiable:**  Inception model should confidently classify the generated images into specific classes (high confidence probabilities).
        2.  **Diverse:** The distribution of predicted classes across many generated images should be diverse (high entropy of class probabilities).
    *   **Equation (Simplified):**
        $$ IS = \exp(\mathbb{E}_{x \sim p_g} [D_{KL}(p(y|x) || p(y))]) $$
        Where:
            *   $x \sim p_g$ : Generated image sampled from Generator's distribution $p_g$.
            *   $p(y|x)$ :  Class probability distribution predicted by Inception for image $x$.
            *   $p(y)$ : Marginal distribution of predicted classes across all generated images.
            *   $D_{KL}$ : Kullback-Leibler divergence (measures the difference between two probability distributions).
            *   $\mathbb{E}$ : Expected value.
            *   $\exp$ : Exponential function.
    *   **Interpretation:** Higher Inception Score is generally better. It reflects both the quality (clarity, realism - related to classifiability) and diversity of generated images.
    *   **Limitations:** Inception Score has been criticized for not always correlating well with visual quality, and it only measures certain aspects of image quality. It can be manipulated (GANs can be designed to "fool" the Inception model).

*   **FrÃ©chet Inception Distance (FID):**

    *   **Concept:**  Also uses the Inception model, but instead of class probabilities, it compares the *feature representations* of real and generated images in the Inception feature space. It assumes that Inception features of real and high-quality generated images should have similar statistical distributions.
    *   **Equation (Simplified):**
        $$ FID = d(m_r, m_g)^2 + Tr(C_r + C_g - 2\sqrt{C_r C_g}) $$
        Where:
            *   $m_r, C_r$ : Mean and covariance matrix of Inception features for real images.
            *   $m_g, C_g$ : Mean and covariance matrix of Inception features for generated images.
            *   $d(m_r, m_g)$ : Euclidean distance between mean vectors.
            *   $Tr$ : Trace of a matrix.
            *   $\sqrt{C_r C_g}$ : Matrix square root.
    *   **Interpretation:** Lower FID is generally better. FID is considered to be more consistent with human visual evaluation than Inception Score. It measures the distance between the distributions of real and generated image features.
    *   **Limitations:** Still relies on the Inception model's feature space, and might not capture all aspects of visual quality. Can be computationally intensive to calculate, especially for large datasets.

*   **Other Metrics:** There are other metrics being developed, like Kernel MMD, Precision and Recall for GANs, but Inception Score and FID are the most widely used for image GAN evaluation.

**3. Analyzing for Mode Collapse:**

*   **Visual Inspection for Lack of Diversity:** If you see that your GAN is generating very similar samples repeatedly, or only generating samples from a limited subset of the data distribution, it's a sign of mode collapse.
*   **Monitoring Diversity Metrics:** Some metrics attempt to quantify diversity explicitly. For example, "precision and recall for GANs" tries to measure how well the generated distribution covers the modes of the real data distribution (recall) and how many generated samples are actually realistic (precision).
*   **Strategies to Mitigate Mode Collapse:** Techniques to reduce mode collapse include:
    *   **Modifying Loss Functions:** Using Wasserstein GAN loss, least squares GAN loss, or other alternative loss functions.
    *   **Adding Regularization:** Techniques like spectral normalization in the Discriminator, or adding noise to Discriminator inputs.
    *   **Improving Training Stability:** Careful hyperparameter tuning, using batch normalization, and other techniques to stabilize GAN training.

**4. Hypothesis Testing / AB Testing (for Model Comparison):**

Similar to other machine learning models, you can use hypothesis testing to compare different GAN architectures or training techniques.

*   **Procedure:**
    1.  Train two different GAN models (A and B) - for example, with different architectures or hyperparameters.
    2.  Generate a set of samples from each model.
    3.  Calculate quantitative metrics (like FID or Inception Score) for the generated samples from both models.
    4.  Use statistical tests (like t-tests or non-parametric tests) to determine if the difference in metric scores between model A and model B is statistically significant.
    5.  Combine quantitative results with visual inspection for a comprehensive comparison.

*   **Example Hypothesis (comparing FID):**
    *   Null Hypothesis (H0): There is no significant difference in FID scores between GAN model A and GAN model B.
    *   Alternative Hypothesis (H1): There is a significant difference in FID scores, and (for example) model B has a lower (better) FID score than model A.

**Important Note on GAN Evaluation:** GAN evaluation is still an open research problem.  Current metrics have limitations, and visual inspection remains a critical part of assessing GAN performance.  Don't rely solely on quantitative metrics; always complement them with qualitative evaluation and domain expertise.

## Tweakable Parameters and Hyperparameter Tuning in GANs

GANs are notoriously sensitive to hyperparameters. Careful tuning is often essential to achieve good results and stable training. Here are some key parameters and hyperparameters to consider:

**Tweakable Parameters (Model Architecture):**

*   **Network Depth and Width (Layers and Units):**
    *   **Effect:**  Deeper and wider networks (more layers and neurons per layer) can potentially learn more complex data distributions. However, they also increase the risk of overfitting and can make training more challenging and computationally expensive.
    *   **Tuning:** Start with relatively shallow networks and gradually increase depth/width. Monitor training progress and validation performance (if you have a validation set for GANs, which is less common than for discriminative models but can be useful in some cases).
*   **Activation Functions:**
    *   **Generator Output Layer:**
        *   `tanh`: Often used when normalizing data to [-1, 1] range.
        *   `sigmoid`: For data normalized to [0, 1] range.
        *   `linear` (no activation) + appropriate loss function (e.g., WGAN loss).
    *   **Hidden Layers (Generator and Discriminator):**
        *   `ReLU` (Rectified Linear Unit): Common and often a good default choice.
        *   `LeakyReLU`: Variant of ReLU that can help prevent "dying ReLU" problem and might improve GAN training in some cases.
        *   `ELU` (Exponential Linear Unit).
    *   **Discriminator Output Layer:**
        *   `sigmoid`: Standard for binary classification (real vs. fake).
    *   **Tuning:** Experiment with different activation functions, especially in hidden layers (ReLU vs. LeakyReLU). For Generator output, choose based on your data normalization range.
*   **Batch Normalization:**
    *   **Effect:** Batch Normalization (BatchNorm) can greatly stabilize GAN training, especially for deeper networks. It normalizes the activations of each layer within each mini-batch, which helps with gradient flow and reduces internal covariate shift.
    *   **Usage:** Commonly used in both Generator and Discriminator networks in GANs.
    *   **Tuning:** Generally, using BatchNorm in GANs is beneficial, but you might experiment with *where* to place BatchNorm layers (e.g., in Generator, in Discriminator, or both, and after which layers).

**Hyperparameters (Training Process):**

*   **Learning Rate (for Generator and Discriminator):**
    *   **Effect:** Learning rate is critical. GANs are sensitive to learning rates. Too high a learning rate can lead to instability, oscillations, and divergence. Too low a learning rate can make training very slow. Often, different learning rates are used for the Generator and Discriminator.
    *   **Tuning:** Start with relatively small learning rates (e.g., 0.0002, 0.0001) and try different values. Adam optimizer is frequently used for GANs.
*   **Optimizer (for Generator and Discriminator):**
    *   **Common Options:** `Adam`, `RMSprop`, `SGD` (Stochastic Gradient Descent).
    *   **Adam:**  Often a good default choice for GANs. Tune `beta_1` parameter (momentum term) in Adam (e.g., set to 0.5 as in DCGAN paper).
    *   **RMSprop:** Another adaptive optimizer that can work well.
    *   **SGD:** Can be more challenging to tune for GANs compared to adaptive optimizers like Adam or RMSprop.
    *   **Tuning:** Experiment with Adam and RMSprop first. Tune learning rates for the chosen optimizer.
*   **Batch Size:**
    *   **Effect:** Batch size affects training stability and speed. Larger batch sizes can provide more stable gradient estimates, but might require more memory. Smaller batch sizes can introduce more noise, which can sometimes help escape local minima but might also make training less stable.
    *   **Tuning:** Try different batch sizes (e.g., 32, 64, 128). Choose a batch size that fits your GPU memory and provides reasonably stable training.
*   **Loss Function:**
    *   **Standard GAN Loss (Binary Crossentropy):**  The original GAN loss discussed earlier.
    *   **Wasserstein GAN (WGAN) Loss:** Designed to improve training stability and address issues like mode collapse. WGAN uses a different distance metric (Earth Mover's Distance or Wasserstein distance) instead of just binary crossentropy. Requires using weight clipping or gradient penalty in the Discriminator.
    *   **Least Squares GAN (LSGAN) Loss:** Uses mean squared error loss instead of binary crossentropy in the Discriminator. Can sometimes lead to more stable training and higher quality samples.
    *   **Tuning:** Experiment with different loss functions, especially if you encounter training instability or mode collapse with the standard GAN loss. WGAN and LSGAN are popular alternatives.
*   **Noise Vector Dimensionality (Latent Dim):**
    *   **Effect:** The dimensionality of the random noise vector `z` (latent space) affects the capacity of the Generator and the diversity of generated samples. Higher dimensionality can potentially allow for more complex and diverse outputs but might also make training more challenging.
    *   **Tuning:** Try different latent dimensions (e.g., 100, 128, 256). Choose a value that is large enough to capture sufficient complexity but not so large that training becomes overly difficult.
*   **Number of Discriminator Updates per Generator Update (k):**
    *   **Concept:** In some GAN implementations, the Discriminator is trained for `k` iterations for every 1 iteration of Generator training (e.g., k=1, k=5). This can help to keep the Discriminator slightly "ahead" of the Generator during training, which can sometimes improve stability.
    *   **Tuning:** Experiment with different values of `k` (e.g., k=1, k=2, k=5).

**Hyperparameter Tuning Techniques:**

*   **Manual Tuning and Grid Search:** Start by manually experimenting with a few key hyperparameters (learning rates, batch size, maybe latent dimension). You can also perform a simple grid search over a small range of values for these hyperparameters.
*   **Random Search:**  If you have a larger hyperparameter space to explore, random search can be more efficient than grid search. Randomly sample hyperparameter combinations and evaluate performance.
*   **Automated Hyperparameter Tuning (e.g., Keras Tuner, Hyperopt, Ray Tune):** For more systematic and efficient tuning, consider using automated hyperparameter tuning tools. These tools can use more advanced search algorithms (like Bayesian optimization, evolutionary algorithms) to find good hyperparameter configurations.

**Example Code Snippet (Demonstrating Hyperparameter Tuning - Conceptual):**

```python
# Conceptual example - NOT fully runnable as is, requires a tuning framework (e.g., Keras Tuner)

def build_and_train_gan(learning_rate_g, learning_rate_d, latent_dim, num_epochs): # Function encapsulating GAN creation and training
    generator = define_generator(latent_dim)
    discriminator = define_discriminator()
    gan_model = define_gan(generator, discriminator, learning_rate_g, learning_rate_d) # Assuming define_gan now takes learning rates

    history = train_gan(generator, discriminator, gan_model, latent_dim, n_epochs=num_epochs) # Train with given hyperparameters
    fid_score = calculate_fid(generator) # Assume calculate_fid function exists
    return fid_score, generator # Return performance metric (FID) and trained generator

# Example hyperparameter values to try
learning_rates_g = [0.0001, 0.0002, 0.00005]
learning_rates_d = [0.0001, 0.0002, 0.00005]
latent_dims = [100, 128]
num_epochs_list = [3000, 5000]

best_fid = float('inf')
best_generator = None
best_hparams = None

for lr_g in learning_rates_g:
    for lr_d in learning_rates_d:
        for ld in latent_dims:
            for epochs in num_epochs_list:
                print(f"Training with lr_g={lr_g}, lr_d={lr_d}, latent_dim={ld}, epochs={epochs}...")
                fid, generator_model = build_and_train_gan(lr_g, lr_d, ld, epochs)
                print(f"FID Score: {fid:.4f}")
                if fid < best_fid: # Assuming lower FID is better
                    best_fid = fid
                    best_generator = generator_model
                    best_hparams = {'lr_g': lr_g, 'lr_d': lr_d, 'latent_dim': ld, 'epochs': epochs}

print("Best Hyperparameters:", best_hparams)
print("Best FID Score:", best_fid)
# Best Generator model is now in 'best_generator'
```

**(Note:** The code above is a conceptual example and would need to be adapted to use a specific hyperparameter tuning framework and evaluation metrics like FID for a complete, runnable hyperparameter search.)

GAN hyperparameter tuning is often an iterative process. Start with a reasonable baseline configuration (e.g., DCGAN architecture with common hyperparameter settings), and then systematically experiment with different hyperparameters, monitoring training stability, sample quality, and quantitative metrics to find a configuration that works well for your specific task and dataset.

## Model Productionizing Steps for GANs

Productionizing GANs is different from deploying discriminative models because the primary output of a GAN is the *generated data* itself, not just predictions for given inputs.  Here are steps for productionizing GANs, focusing on generating and making generated data available:

**1. Local Testing and Sample Generation Refinement:**

*   **Notebook/Scripts for Sample Exploration:** Continue using notebooks or scripts to experiment with generating samples from your trained Generator. Visualize samples, evaluate quality qualitatively, and refine the generation process.
*   **Sample Batch Generation Script:** Create a script that efficiently generates batches of samples from the Generator and saves them (e.g., as images, data files). This script will be the core of your generation pipeline.
*   **Conditioning/Control (if applicable):** If your GAN is conditioned (e.g., Conditional GANs - CGANs, StyleGAN with style inputs), refine the process of providing condition inputs to control the generation process. Test different conditions and ensure they produce the desired variations in the generated data.

**2. On-Premise/Cloud Sample Generation Pipeline:**

*   **Server/Compute Environment:** Set up a server or cloud compute instance with GPUs (if needed for generation speed, especially for image GANs) where your sample generation script will run.
*   **Batch Sample Generation Job:** Schedule or manually trigger your sample generation script to run on this server. This could be a one-time job to generate a dataset of samples, or a recurring job to continuously generate new samples.
*   **Data Storage:** Configure storage for the generated data (e.g., local file system, network storage, cloud storage like AWS S3, Google Cloud Storage).

**3. API for On-Demand Sample Generation (Optional but Useful for some applications):**

For use cases where you need to generate samples on demand (e.g., a user request, real-time generation), you can create an API around your Generator.

*   **API Framework (Flask/FastAPI):** Use frameworks like Flask or FastAPI to build a web API. The API endpoint will receive requests (potentially with conditions if using a conditional GAN).
*   **API Logic:** The API logic will:
    1.  Receive input requests (noise vectors or condition inputs).
    2.  Pass the input to the loaded Generator model.
    3.  Get generated samples as output.
    4.  Return the generated samples in the API response (e.g., as image data, data file download link, etc.).
*   **Containerization (Docker):** Package your API application (including the Generator model and dependencies) into a Docker container for consistent deployment.
*   **Deployment Platform (Cloud or On-Premise):** Deploy the containerized API to a cloud platform (AWS ECS, Google Cloud Run, Azure Container Instances, etc.) or on-premise server infrastructure.
*   **Load Balancing and Scaling (if needed):** For high-demand APIs, use load balancers and scale out API instances to handle traffic.

**4. Cloud-Based Managed GAN Platforms (Emerging but not as mature as for discriminative models):**

Cloud platforms are starting to offer more managed services for generative models, but the ecosystem is still developing.

*   **AWS SageMaker (Limited Support for Generative Models):**  SageMaker is primarily focused on discriminative models, but you can use SageMaker to deploy your custom GAN training and inference pipelines.
*   **Google AI Platform (Vertex AI):** Similar to SageMaker, Vertex AI can be used to manage custom GAN workflows.
*   **Specialized Generative AI Platforms (Emerging):** Some platforms are emerging that are more specifically focused on generative AI, potentially offering easier deployment and management of GANs and other generative models, but these are generally less mature than platforms for traditional ML models.

**Code Snippet: Example Flask API for On-Demand Sample Generation (Conceptual):**

```python
# Conceptual Flask API example - Requires installation of Flask, and a trained & saved generator model
from flask import Flask, request, jsonify
import numpy as np
from tensorflow.keras.models import load_model
import io
from PIL import Image
import base64

app = Flask(__name__)

# Load your trained generator model (replace 'generator_model_path' with actual path)
generator = load_model('generator_model')
latent_dim = 10 # Assuming latent_dim is 10 - adjust accordingly

@app.route('/generate_image', methods=['POST'])
def generate_image_endpoint():
    try:
        noise_vector = np.random.randn(1, latent_dim) # Generate noise for one sample
        generated_sample = generator.predict(noise_vector)

        # Assuming generated_sample is image data (adjust denormalization as needed)
        generated_image_array = ((generated_sample[0] + 1) * 127.5).clip(0, 255).astype(np.uint8) # Denormalize to 0-255
        image = Image.fromarray(generated_image_array)

        # Encode image to base64 for JSON response
        buffered = io.BytesIO()
        image.save(buffered, format="JPEG") # Or PNG
        img_str = base64.b64encode(buffered.getvalue()).decode()

        return jsonify({'image_base64': img_str}), 200

    except Exception as e:
        return jsonify({'error': str(e)}), 500 # Return error response

if __name__ == '__main__':
    app.run(debug=False, host='0.0.0.0', port=5000) # Run Flask app
```

**(Note:** This Flask API code is a simplified conceptual example and would need to be adapted based on your specific GAN model, data type, desired API functionality, and security requirements. For image data, you'd need to handle image encoding/decoding, and adjust denormalization steps accordingly.)

**Productionizing GAN Considerations:**

*   **Scalability of Generation:** Optimize the Generator model and generation code for speed if you need to generate a large number of samples quickly or in real-time. GPUs can be beneficial for image generation.
*   **Sample Storage and Management:** Plan for storage and management of generated data. Consider data versioning, metadata, and efficient data access if you are generating large datasets.
*   **Monitoring and Maintenance:** Monitor the performance of your generation pipeline or API (if deployed). Track error rates, generation times, and ensure the system is running smoothly. Periodically regenerate samples or retrain the GAN if the quality of generated data degrades over time (concept drift).
*   **Ethical Considerations:** For applications where GANs generate content that could be sensitive or have societal impact (e.g., deepfakes, synthetic media), carefully consider ethical implications and potential misuse. Implement safeguards and responsible AI practices.

The productionizing steps for GANs are tailored to the specific use case and whether you need to generate samples in batch, on-demand via API, or as part of a larger data pipeline. Focus on efficient sample generation, data storage, and monitoring to ensure your GAN system operates reliably in a production environment.

## Conclusion: GANs - The Creative Engines of AI and Beyond

Generative Adversarial Networks (GANs) have emerged as a powerful and transformative class of algorithms in machine learning. Their ability to learn complex data distributions and generate novel, realistic data has opened up exciting possibilities across diverse fields.

**Real-World Applications and Ongoing Impact:**

GANs are not just a theoretical concept; they are being applied and impacting real-world applications:

*   **Creative Industries (Art, Design, Entertainment):** GANs are tools for artists, designers, and content creators, enabling new forms of artistic expression, style transfer, content generation for games, and special effects in movies.
*   **Image and Video Editing/Enhancement:** GANs are used in image super-resolution, image inpainting, video frame interpolation, and other tasks that enhance and manipulate visual content.
*   **Medical Imaging:** GANs are explored for generating synthetic medical images for training diagnostic models, denoising medical scans, and even for medical image translation (e.g., MRI to CT scans).
*   **Drug Discovery and Material Science:** GANs are used for generating novel molecules with desired properties for drug candidates and for designing new materials with specific characteristics.
*   **Fashion and Product Design:** GANs can aid in generating new fashion designs, product concepts, and variations of existing products.
*   **Data Augmentation and Synthetic Data Generation:** GANs are used to create synthetic datasets to augment training data for other machine learning models, especially in situations where real data is scarce or sensitive.

**Optimized and Newer Generative Models:**

The field of generative modeling is rapidly evolving. While GANs have been highly influential, newer and optimized generative models are continuously being developed:

*   **Variational Autoencoders (VAEs):** VAEs are another type of generative model, often seen as complementary to GANs. VAEs are generally easier to train and sample from but might sometimes generate samples of slightly lower visual quality compared to GANs. Research combines VAEs and GANs for hybrid models.
*   **Normalizing Flows:** Normalizing flow models offer a different approach to generative modeling, based on transforming simple probability distributions into complex ones through a series of invertible mappings. Flows can be easier to train than GANs and provide exact likelihood computation, but might sometimes struggle to capture very complex distributions.
*   **Diffusion Models (e.g., DALL-E 2, Stable Diffusion):** Diffusion models have recently achieved state-of-the-art results in image generation and are gaining significant attention. They work by learning to reverse a gradual noising process. Diffusion models often produce very high-quality and diverse samples and can be more stable to train than GANs.
*   **Transformers for Generative Modeling:** Transformer architectures, which have revolutionized NLP, are also being adapted for generative tasks beyond text, including image generation (e.g., image Transformers, Vision Transformers for generation).

**The Future of Generative AI:**

Generative AI is a dynamic and rapidly advancing field. We can expect to see continued progress in:

*   **Improved Sample Quality and Resolution:** Generative models will continue to produce even more realistic and higher-resolution data across various modalities (images, video, audio, 3D models).
*   **Enhanced Control and Conditionality:**  Models will become better at generating data that is more controllable and conditioned on specific user inputs or desired attributes.
*   **Greater Training Stability and Efficiency:** Research is ongoing to develop more stable and efficient training methods for GANs and other generative models, reducing the "art" and increasing the "science" of training.
*   **Broader Applications:** Generative AI will likely expand its reach into even more application areas, transforming industries and creating new possibilities across art, science, and technology.

GANs have been a pivotal step in the journey of generative AI. They have demonstrated the power of adversarial learning and opened up a new era of machine creativity. As research progresses and new algorithms emerge, generative models will continue to reshape how we interact with AI and create in the digital world.

---

## References

1.  **Goodfellow, I. J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., ... & Bengio, Y. (2014). Generative adversarial nets.** *Advances in neural information processing systems*, *27*. [Original GAN paper](https://proceedings.neurips.cc/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf)
2.  **Radford, A., Metz, L., & Chintala, S. (2015). Unsupervised representation learning with deep convolutional generative adversarial networks.** *arXiv preprint arXiv:1511.06434*. [DCGAN paper - Deep Convolutional GANs, a popular architecture](https://arxiv.org/abs/1511.06434)
3.  **Arjovsky, M., Chintala, S., & Bottou, L. (2017). Wasserstein generative adversarial networks.** *In International conference on machine learning* (pp. 214-223). PMLR. [WGAN paper - Wasserstein GANs, improved training stability](https://proceedings.mlr.press/v70/arjovsky17a.html)
4.  **Salimans, T., Goodfellow, I., Zaremba, W., & Sutskever, I. (2016). Improved techniques for training GANs.** *Advances in neural information processing systems*, *29*. [Paper on techniques to improve GAN training](https://proceedings.neurips.cc/paper/2016/file/8a3363abe792db2d878e4fba69b96ba5-Paper.pdf)
5.  **Olah, C. (2014). Generative Models.** *Colah's Blog*. [Blog post explaining generative models, including GANs](https://colah.github.io/generative-models/)
6.  **Goodfellow, I., Bengio, Y., & Courville, A. (2016). *Deep learning*.** MIT press. [Deep Learning textbook, chapter on generative models including GANs](https://www.deeplearningbook.org/)
7.  **Keras Documentation on GANs:** [Keras GAN example and guide](https://keras.io/examples/generative/dcgan_vae/) (and other GAN examples in Keras documentation)
8.  **PyTorch GAN Tutorial:** [PyTorch GAN tutorial using torchvision](https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html)
9.  **Lucic, M., Kurach, K., Gelly, S., Bousquet, O., & V